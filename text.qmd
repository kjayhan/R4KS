# Korean Text Analysis

In this chapter, we will learn how to analyze Korean text data using R. Korea's 2024 Public Diplomacy Comprehensive Implementation Plan (2024년 공공외교 종합시행계획 개요) as an example text.

We will learn the following things in order:

-   Extracting text and tables from a PDF file.
-   Extracting text and tables from the internet.
-   Ensuring accurate spacing between words in Korean text.
-   Analyzing morphemes in Korean text.
-   Analyzing word frequency in Korean text.
-   Analyzing the noun word network in Korean text.
-   Analyzing the sentiment of Korean text.
-   Topic modeling of Korean text.
-   Korean tweet analysis.

## Libraries

First, we need to install [`bitNLP`](https://r2bit.com/bitNLP/) which requires us to install the [`MeCab`](https://taku910.github.io/mecab/) library for Korean text analysis. Uncomment the following lines in your first usage. After the first usage, you can comment out the installation lines.

```{r}
# install.packages("remotes")
# remotes::install_github("bit2r/bitNLP")
library(bitNLP)
# install_mecab_ko()
# install.packages("RcppMeCab")
```

Now let's load the necessary libraries. If you are missing any of the following packages, you can install them by uncommenting the `install.packages` lines.

```{r}
# install.packages("tidyverse")
# install.packages("pdftools")
# install.packages("rvest")
# install.packages("tidytext")
# install.packages("igraph")
# install.packages("ggraph")
# install.packages("extrafont")
# install.packages("devtools")
# devtools::install_github("koheiw/stopwords")
library(tidyverse)
library(pdftools)
library(rvest)
library(tidytext)
library(igraph)
library(ggraph)
library(extrafont)
library(stopwords)
```

## Loading pdf Data

Let's analyze the text from Korea's 2024 Public Diplomacy Comprehensive Implementation Plan (2024년 공공외교 종합시행계획 개요) which is available as a pdf file on the Ministry of Foreign Affairs' (MOFA) [website](https://www.mofa.go.kr/cntntsDown.do?path=www&physic=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf&real=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf)[^text-1].

[^text-1]: Please bear in mind that MOFA website's url might change later, making this hyperlink broken. In that case, you can download the pdf file on the MOFA's website by searching for "2024년 공공외교 종합시행계획 개요".

If the pdf file is in your local directory, you can load it using the following code.

```{r}
# Load PDF
pdf_path <- "data/2024공공외교.pdf"

```

Alternatively, you can download the pdf file from the MOFA's website using the [`download.file`](https://henrikbengtsson.github.io/R.utils/reference/downloadFile.character.html) function. You can then load the pdf file using the `pdf_path` variable. Working with the online pdf file and the local pdf file is the same. We can do either. For now, I will use the local pdf file since the MOFA might change the url for the pdf later. That is why I commented the download code. You can comment the earlier code for the local pdf file and uncomment the following code for the online pdf file.

```{r}
# Download PDF
#file <- tempfile()

# This url works for now. But MOFA might change it later. You can replace the link with any other link you want to download.

#url <- "https://www.mofa.go.kr/cntntsDown.do?path=www&physic=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf&real=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf"

# download.file(url, pdf_path, headers = c("User-Agent" = "My Custom User Agent"))
```

Now let's extract the text from the pdf file using the [`pdf_text`](https://docs.ropensci.org/pdftools/reference/pdftools.html) function from the [`pdftools`](https://docs.ropensci.org/pdftools/index.html) package.

```{r}
# Extract text
pdf_text_all <- pdf_text(pdf_path)
```

Now, `pdf_text_all` is a list of character vectors, where each element corresponds to a page in the pdf file. For example, we can look at the 4^th^ page of the pdf file in the following way.

```{r}
# Let's look at the 4th page
pdf_text_all[4]
```

You can see that there are `\n` characters, which refers to newline (new line) in the text. Let's split the text by the newline character and look at the first 10 lines of the 4th page. We can split the text into lines by using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package, which is part of [`tidyverse`](https://www.tidyverse.org/). So, we do not need to load it separately. Let's look at the first six lines of the 4th page.

```{r}
# Look at the first 10 lines of the 4th page
pdf_text_all[4] |> 
  # Split by newline character.
  str_split("\n") |> 
  # Unlist
  unlist() |>
  # Take the first 10 lines
  head(10)
```

The 4th page in the pdf file looks like this:

![2024 Public Diplomacy Comprehensive Implementation Plan, p. 4](data/2024pd4.png)

## pdf Table Extraction

Let's try to extract the second table on page 4 of the pdf file. The table has the number of public diplomacy projects and budgets for [first-tier local administration unit](https://www.mois.go.kr/eng/sub/a03/citiesprovinces/screen.do) (hereafter, *province_city* for short) in Korea. We will unlist each line as we did earlier so that we can see the table in a more readable way.

```{r}
# Look at the first 10 lines of the 4th page
lines_pdf_4 <- pdf_text_all[4] |> 
  # Split by newline character.
  str_split("\n") |> 
  # Unlist
  unlist()
```

First, let's look at the 29^th^ and 30^th^ lines for the column names in the pdf file.

```{r}
lines_pdf_4[29:30]
```

The column names are the line number, province or city's name, project numbers for 2024 and 2023 respectively, and the budget for 2024 and 2023 in million Korean Won respectively. Let's use the following English column names that correspond to the Korean column names in the pdf file.

```{r}
# Column names
col_names <- c("no", "province_city", "project_no_2024", "project_no_2023", "budget_2024", "budget_2023")
```

By observing the `lines_pdf_4` object using `view(lines_pdf_4)`, we can see that the second table starts from the 32^nd^ line and ends on the 48^th^. We will extract only those lines. We will use [`str_trim`](https://stringr.tidyverse.org/reference/str_trim.html) "removes whitespace from start and end of string". We will also use [`str_replace_all`](https://stringr.tidyverse.org/reference/str_replace.html) to remove commas from each line to convert entries into numbers. We will then split each line based on two or more consecutive spaces (our string is "`\s{2,}`") using [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) and simplify the result into a matrix. We will convert this matrix into a data frame with non-factor columns using `data.frame(stringsAsFactors = FALSE)`. We will set the column names of the data frame using the `col_names` vector that we created above. These explanations are also available in each step in the following code chunk.

```{r}
# Select lines 32 to 48 from the lines_pdf_4 data frame
province_city_pd <- lines_pdf_4[32:48] |>
  # Trim whitespace from both ends of each element in the selected rows
  str_trim() |>
  # Replace all commas with an empty string in each element
  str_replace_all(",", "") |>
  # Split each element based on 2 or more consecutive spaces and simplify into a matrix
  str_split("\\s{2,}", simplify = TRUE) |>
  # Convert the matrix into a data frame with non-factor columns
  data.frame(stringsAsFactors = FALSE) |>
  # Set column names for the data frame using the provided 'col_names' vector
  setNames(col_names)
```

Let's rearrange the table (which is originally in alphabetical order) by descending order based on public diplomacy budgets in 2024.

```{r}
province_city_pd |>
  arrange(desc(budget_2024))
```

But these province_city names are in Korean since the document was in Korean. Let's practice extracting a table from internet then to find English names for these Korean provinces or cities. As of May 6, 2024, [Wikipedia's list of South Korea's administrative divisions](https://en.wikipedia.org/wiki/Administrative_divisions_of_South_Korea) seems to be correct. Let's extract the table there.

## html Table Extraction

We will use the [`rvest`](https://rvest.tidyverse.org/) package to extract the table from the Wikipedia page. We will use the [`read_html`](https://rvest.tidyverse.org/reference/read_html.html) function to read the html content of the Wikipedia page. We will then use the [`html_node`](https://rvest.tidyverse.org/reference/html_nodes.html) function to select the table we want to extract. You can refer to [`rvest`](https://rvest.tidyverse.org/) package for more information on how to extract what you want. We can use the xpath of the table we want to extract. You can find the xpath of the table by right-clicking on the table on the Wikipedia page and selecting "Inspect" or "Inspect Element" depending on your browser. You can then right-click on the highlighted html element in the "Elements" tab of the "Developer Tools" and select "Copy" -\> "Copy XPath". The xpath of the table we want to extract is `//*[@id="mw-content-text"]/div[1]/table[5]`. We will use the [`html_table`](https://rvest.tidyverse.org/reference/html_table.html) function to extract the table as a data frame. We will use the `fill = TRUE` argument to fill in the missing values in the table.

```{r}
html <- read_html("https://en.wikipedia.org/wiki/Administrative_divisions_of_South_Korea")

table <- html |> 
  html_node(xpath = '//*[@id="mw-content-text"]/div[1]/table[5]') |>
  html_table(fill = TRUE)
```

Let's look at the first 10 rows of the table.

```{r}
head(table)
```

Perfect! Now, let's keep only the columns that we will need.

```{r}
# Select columns 4 and 5 from the table
table <- table |>
  select(4:5)

# Let's change the English province_city column name.

table <- table |>
  rename(province_city_eng = `Official English name[5]`)
```

Let's hope that the Korean names in the Wikipedia table and the MOFA's pdf file are the same. Let's merge the two tables based on the Korean names.

```{r}
# Merge the two tables based on the Korean names
province_city_pd_joined <- province_city_pd |>
  left_join(table, by = c("province_city" = "Hangul"))
```

Let's see if we have any missing values in the English names.

```{r}
# Check for missing values in the English names
province_city_pd_joined |>
  filter(is.na(province_city_eng))
```

We almost got it! The only difference is 전라북도 (North Jeolla Province) in the MOFA's pdf file which is written as 전북특별자치도 (Jeonbuk State) in the Wikipedia table. Let's fix this.

```{r}
# Move the English name column next to the Korean name column, and remove the 'no' column

province_city_pd_joined <- province_city_pd_joined |>
  select(province_city, province_city_eng, everything(), -no)

# Fix the English name of 전라북도

province_city_pd_joined <- province_city_pd_joined |>
  mutate(province_city_eng = ifelse(province_city == "전라북도", "North Jeolla province_city", province_city_eng))
```

## Text Analysis

### Word Frequency

This time let's look at all of the text in the 2024 Public Diplomacy Comprehensive Implementation Plan. We will combine all the text into a single character vector.

```{r}
# Combine text
pdf_text <- str_c(pdf_text_all, collapse = " ")
```

We will now split the text into words using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package. We will then convert the result into a data frame with non-factor columns using the [`data.frame(stringsAsFactors = FALSE)`](https://rdocumentation.org/packages/base/versions/3.6.2) function. We will set the column name of the data frame as `word`.

```{r}
# Split the text into words
words <- pdf_text |>
  # Split the text into words
  str_split("\\s+") |>
  # Convert the result into a data frame with non-factor columns
  data.frame(stringsAsFactors = FALSE) |>
  # Set the column name of the data frame as "word"
  setNames("word")
```

Let's look at the first 10 rows of the data frame.

```{r}
head(words, 10)
```

Now, let's count the frequency of each word in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package. We will then arrange the result in descending order based on the frequency of the words.

```{r}
# Count the frequency of each word
word_freq <- words |>
  count(word, sort = TRUE)
```

Let's look at the first 10 rows of the data frame

```{r}
head(word_freq, 10)
```

This is not very useful. There are two main issues with Korean text. First, Korean text does not have consistent spacing between words. Second, Korean text has particles and other morphemes that are not words. We will address these issues now.

### Spacing in Korean Text

Let's get the spacing right in Korean text using the [`bitNLP`](https://r2bit.com/bitNLP) package's [`get_spacing`](https://r2bit.com/bitNLP/reference/get_spacing.html) function, which will add spaces between words in the Korean text. So, for example "한국공공외교" will become "한국 공공 외교".

```{r}
# Get the spacing right in Korean text
pdf_text_ko <- get_spacing(pdf_text)
```

Now, let's split the text into words again using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package.

```{r}
# Split the text into words
words_ko <- pdf_text_ko |>
  # Split the text into words
  str_split("\\s+") |>
  # Convert the result into a data frame with non-factor columns
  data.frame(stringsAsFactors = FALSE) |>
  # Set the column name of the data frame as "word"
  setNames("word")
```

Let's analyze the word frequency in the text again.

```{r}
# Count the frequency of each word
word_freq_ko <- words_ko |>
  count(word, sort = TRUE)

head(word_freq_ko, 10)
```

We have many special characters in the text. Let's remove all characters except for Korean characters, spaces, English letters, and numbers using the [`str_replace_all`](https://stringr.tidyverse.org/reference/str_replace.html) function from the [`stringr`](https://stringr.tidyverse.org/) package.

```{r}
# Remove all characters except for Korean characters, spaces, English letters, and numbers
word_freq_ko <- pdf_text_ko |>
  # Remove all characters except Korean characters, English letters, numbers, and spaces
  str_replace_all("[^가-힣a-zA-Z0-9\\s]", "") |>
  # Split the cleaned text into words based on one or more spaces
  str_split("\\s+") |>
  # Convert the list result into a data frame with non-factor columns
  data.frame(stringsAsFactors = FALSE) |>
  # Set the column name of the data frame as "word"
  setNames("word")
```

Let's analyze the word frequency in the text again.

```{r}
# Count the frequency of each word
word_freq_ko <- word_freq_ko |>
  count(word, sort = TRUE)

head(word_freq_ko, 10)
```

This is much better! We have removed the special characters and have more meaningful words in the text.

We can also remove some common stopwords in Korean using the [`stopwords`](https://github.com/koheiw/stopwords) function in [`stopwords`](https://github.com/koheiw/stopwords) package along with the [`stopwords-iso`](https://github.com/stopwords-iso/stopwords-iso/) library that has Korean stopwords.

```{r}

stopwords <- stopwords("ko", source = "stopwords-iso") |>
  as.data.frame() |>
  setNames("word")

# Remove stopwords

word_freq_ko <- word_freq_ko |>
  # anti_join (remove) stopwords
  anti_join(stopwords, by = "word")
```

Let's count the frequency of each word in the text again.

```{r}
head(word_freq_ko, 10)
```

This way, we removed words such as 및 (and) and 등 (etc.) from the text.

Let's move on to morpheme analysis which makes more sense in Korean text analysis context.

### Morpheme Analysis in Korean Text

Let's analyze the morphemes in the Korean text using the [`morpho_mecab`](https://r2bit.com/bitNLP/reference/morpho_mecab.html) function from the [`bitNLP`](https://r2bit.com/bitNLP) package, which will extract morphemes from the Korean text.

```{r}
# Analyze the morphemes in the Korean text
morphemes <- morpho_mecab(pdf_text_ko)
```

This creates a list of character vectors, where each element corresponds to a morpheme in the text. We can also combine all of the morphemes and tokenize them into a single character vector.

```{r}
# Combine all the morphemes into a single character vector

morphemes_single <- morpho_mecab(pdf_text_ko, indiv = FALSE)
```

Now, let's split the text into words again this time by converting `morphemes_single` into a data frame using the [`as.data.frame`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.data.frame) function. We will set the column name of the data frame as "word".

```{r}
# Split the text into words
words_morphemes <- morphemes_single |>
  as.data.frame() |>
  # Set the column name of the data frame as "word"
  setNames("word")
```

We will now count the frequency of each morpheme in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package. We will then arrange the result in descending order based on the frequency of the morphemes.

```{r}
# Count the frequency of each morpheme

morpheme_freq <- words_morphemes |>
  count(word, sort = TRUE)

head(morpheme_freq, 10)
```

Now, this is more like it!

Let's visualize the frequency of the morphemes in the text using a bar plot. Before that let's address the font issue with Korean text in the plot.

Korean text sometimes is not visible in the graph due to the font issue. This was the case in my Macbook. Let's set the font to one that supports Korean characters. We will use the [`extrafont`](https://github.com/wch/extrafont) package to set the font to one that supports Korean characters. We will use the [`font_import`](https://github.com/wch/extrafont) function to import the fonts from the system. This may take some time. You only need to do it once. That's why I commented it. You can uncomment it in first usage.

```{r}
# Load extrafont and register fonts

#font_import()  # This might take a while if it's the first time you're running it
```

We will then use the [`loadfonts`](https://github.com/wch/extrafont) function to load the fonts. We will use the [`fonts`](https://github.com/wch/extrafont) function to display the available fonts and find one that supports Korean characters. We will set the font to one that supports Korean characters. For now, I have chosen "Arial Unicode MS" as the Korean font. You can replace it with a font from your system that supports Korean characters if necessary.

```{r}
#loadfonts(device = "all")

# Display available fonts, find one that supports Korean
#fonts()

# Set the font to one that supports Korean characters
korean_font <- "Arial Unicode MS"  # Replace with a font from your system that supports Korean if necessary
```

We will use the [`ggplot`](https://ggplot2.tidyverse.org/) function from the [`ggplot2`](https://ggplot2.tidyverse.org/) package to create the plot. We will use the [`geom_col`](https://ggplot2.tidyverse.org/reference/geom_col.html) function to add the bars to the plot. We will use the [`theme_minimal`](https://ggplot2.tidyverse.org/reference/theme_minimal.html) function to set the theme of the plot to minimal. We will use the [`theme`](https://ggplot2.tidyverse.org/reference/theme.html) function to adjust the font size in the plot. We will set the font size to 10. We will use the [`labs`](https://ggplot2.tidyverse.org/reference/labs.html) function to add the title and labels to the plot. We will visualize only the most frequent 20 morphemes in the text.

```{r}
# Visualize the frequency of the morphemes in the text

morpheme_freq |> 
  top_n(20) |> 
  mutate(word = reorder(word, n)) |> ggplot(aes(word, n)) + 
  geom_col(fill = "#2196f3") +
  coord_flip() +
  theme_minimal() +
  # use Korean font
  theme(text = element_text(family = korean_font, size = 10)) +
  labs(title = "Frequency of Morphemes in Korean Text", x = "Morpheme", y = "Frequency")
```

### Word Network in Korean Text

Let's analyze the word network in the Korean text using the [`tokenize_noun_ngrams`](https://r2bit.com/bitNLP/reference/tokenize_noun_ngrams.html) function from the [`bitNLP`](https://r2bit.com/bitNLP/) package which builds on [`tidytext`](https://r2bit.com/bitNLP/articles/with_tidytext.html) package. We will use the `tokenize_noun_grams` function to extract the noun word network from the Korean text.

```{r}
# We can use a user-defined dictionary to improve the accuracy of the tokenization. We will rely on the one provided by the `bitNLP` package.

dic_path <- system.file("dic", package = "bitNLP")
dic_file <- glue::glue("{dic_path}/buzz_dic.dic")

word_network <- tokenize_noun_ngrams(pdf_text_ko, simplify = TRUE, user_dic = dic_file, n = 2) |>
  as.data.frame() |>
  setNames("paired_words")
```

Now, let's separate the paired words into two columns using the [`separate`](https://tidyr.tidyverse.org/reference/separate.html) function from the [`tidyr`](https://tidyr.tidyverse.org/) package which is loaded as part of the [`tidyverse`](https://www.tidyverse.org/) package. This will allow us to create bigrams from the paired words.

```{r}
word_network_separated <- word_network |>
  separate(paired_words, c("word1", "word2"), sep = " ")
```

We will now count the frequency of each bigram in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package, which is also party of the [`tidyverse`](https://www.tidyverse.org/). We will then arrange the result in descending order based on the frequency of the bigrams.

```{r}
# new bigram counts:
word_network_counts <- word_network_separated |>
  count(word1, word2, sort = TRUE)
```

We will now create a graph from the bigram counts using the [`graph_from_data_frame`](https://r.igraph.org/reference/graph_from_data_frame.html) function from the [`igraph`](https://r.igraph.org/) package. We will use the [`ggraph`](https://ggraph.data-imaginist.com/reference/ggraph.html) function from the [`ggraph`](https://ggraph.data-imaginist.com) package to create the graph. We will use the [`geom_edge_link`](https://ggraph.data-imaginist.com/reference/geom_edge_link.html) function to add the edges to the graph. We will use the [`geom_node_point`](https://ggraph.data-imaginist.com/reference/geom_node_point.html) function to add the nodes to the graph. We will use the [`geom_node_text`](https://ggraph.data-imaginist.com/reference/geom_node_text.html) function to add the labels to the nodes in the graph. We will set the font to the Korean font that we set earlier. We will then adjust the font in the graph. Here, `n >= 6` is used to filter out bigrams that appear less than 6 times. You can adjust this number as needed. You can check out `ggraph` layout options [here](https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html).

```{r}

word_network_select <- word_network_counts |>
  filter(n >= 6) |>
  graph_from_data_frame() |>
  ggraph(layout = "fr") +
  geom_edge_link(aes()) +
  geom_node_point(color = "#2196f3", size = 4) +
  geom_node_text(aes(label = name), family = korean_font, vjust = 2, size = 4) +  # Set family to Korean font
  theme_void()


word_network_select
```

### Sentiment Analysis

### Topic Modeling

## Korean Tweet Analysis

## Further Readings

## References

## Session Info

```{r}
sessionInfo()
```
