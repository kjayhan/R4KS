{
  "hash": "7531752f0f73e3342382c30e5ffe5522",
  "result": {
    "markdown": "# Korean Text Analysis\n\nIn this chapter, we will learn how to analyze Korean text data using R. We will use the [`tidyverse`](https://www.tidyverse.org/), `pdftools`, and `bitNLP` packages to extract text from a pdf file and analyze it. We will use Korea's 2022 Diplomatic White Paper (외교백서, *waegyo baekseo*) as an example text.\n\nWe will learn the following things in order:\n\n-   Extracting text and tables from a PDF file.\n-   Extracting text and tables from the internet.\n-   Ensuring accurate spacing between words in Korean text.\n-   Analyzing morphemes in Korean text.\n-   Analyzing word frequency in Korean text.\n-   Analyzing the noun word network in Korean text.\n-   Analyzing the sentiment of Korean text.\n-   Topic modeling of Korean text.\n\n## Libraries\n\nFirst, we need to install [`bitNLP`](https://r2bit.com/bitNLP/) which requires us to install the [`MeCab`](https://taku910.github.io/mecab/) library for Korean text analysis. Uncomment the following lines in your first usage. After the first usage, you can comment out the installation lines.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"remotes\")\n# remotes::install_github(\"bit2r/bitNLP\")\nlibrary(bitNLP)\n# install_mecab_ko()\n# install.packages(\"RcppMeCab\")\n```\n:::\n\n\n\nNow let's load the necessary libraries. If you are missing any of the following packages, you can install them by uncommenting the `install.packages` lines.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"tidyverse\")\n# install.packages(\"pdftools\")\n# install.packages(\"rvest\")\n# install.packages(\"tidytext\")\n# install.packages(\"igraph\")\n# install.packages(\"ggraph\")\n# install.packages(\"extrafont\")\nlibrary(tidyverse)\nlibrary(pdftools)\nlibrary(rvest)\nlibrary(tidytext)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(extrafont)\n```\n:::\n\n\n\n## Loading pdf Data\n\nLet's analyze the text from Korea's 2024 Public Diplomacy Comprehensive Implementation Plan (2024년 공공외교 종합시행계획 개요) which is available as a pdf file on the Ministry of Foreign Affairs' (MOFA) [website](https://www.mofa.go.kr/cntntsDown.do?path=www&physic=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf&real=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf)[^text-1].\n\n[^text-1]: Please bear in mind that MOFA website's url might change later, making this hyperlink broken. In that case, you can download the pdf file on the MOFA's website by searching for \"2024년 공공외교 종합시행계획 개요\".\n\nIf the pdf file is in your local directory, you can load it using the following code. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load PDF\npdf_path <- \"data/2024공공외교.pdf\"\n```\n:::\n\n\n\nAlternatively, you can download the pdf file from the MOFA's website using the [`download.file`](https://henrikbengtsson.github.io/R.utils/reference/downloadFile.character.html) function. You can then load the pdf file using the `pdf_path` variable.\nWorking with the online pdf file and the local pdf file is the same. We can do either. For now, I will use the local pdf file since the MOFA might change the url for the pdf later. That is why I commented the download code. You can comment the earlier code for the local pdf file and uncomment the following  code for the online pdf file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Download PDF\n#file <- tempfile()\n\n# This url works for now. But MOFA might change it later. You can replace the link with any other link you want to download.\n\n#url <- \"https://www.mofa.go.kr/cntntsDown.do?path=www&physic=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf&real=2024%EB%85%84%EB%8F%84_%EA%B3%B5%EA%B3%B5%EC%99%B8%EA%B5%90_%EC%A2%85%ED%95%A9%EC%8B%9C%ED%96%89%EA%B3%84%ED%9A%8D.pdf\"\n\n# download.file(url, pdf_path, headers = c(\"User-Agent\" = \"My Custom User Agent\"))\n```\n:::\n\n\n\n\nNow let's extract the text from the pdf file using the [`pdf_text`](https://docs.ropensci.org/pdftools/reference/pdftools.html) function from the [`pdftools`](https://docs.ropensci.org/pdftools/index.html) package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract text\npdf_text_all <- pdf_text(pdf_path)\n```\n:::\n\n\n\nNow, `pdf_text_all` is a list of character vectors, where each element corresponds to a page in the pdf file. For example, we can look at the 4^th^ page of the pdf file in the following way.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's look at the 4th page\npdf_text_all[4]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"참고       기관별 사업규모 및 예산\\n[중앙행정기관]\\n                  ‘24년     ‘23년      ‘24년 예산         ‘23년 예산\\n       기관명\\n                  사업수      사업수         (백만원)           (백만원)\\n 1    교육부             16       16          194,996          94,963\\n 2    과학기술정보통신부        6        6           32,852          40,283\\n 3    외교부             73       63           40,215          39,419\\n3-1   한국국제교류재단        37       41           42,514          44,664\\n 4    통일부              6        6            1,831           2,386\\n 5    법무부              3        3           15,068          14,346\\n 6    국방부              7        8            6,165           7,221\\n 7    행정안전부            3        3              594             574\\n 8    문화체육관광부         21       22          185,478         145,049\\n 9    농림축산식품부          6        7            3,048           4,268\\n10    보건복지부            7        7            6,497           8,557\\n11    환경부              1        1            1,888           1,427\\n12    고용노동부            1        1            1,264           1,529\\n13    여성가족부            6        7            1,531           2,748\\n14    국토교통부            4        4            2,394           2,394\\n15    중소벤처기업부          5        5            7,246           5,548\\n16    국가보훈부            1        1            8,774           3,637\\n17    법제처              2        2              327             327\\n18    해양수산부            1        1              100             100\\n19    재외동포청            5        -           22,289               -\\n        합계           211      204         475,038         419,440\\n\\n[지자체]\\n                  ‘24년     ‘23년      ‘24년 예산         ‘23년 예산\\n       기관명\\n                  사업수      사업수         (백만원)           (백만원)\\n 1    경기도             25       14          21,558            3,899\\n 2    강원특별자치도         10       11          78,593          11,024\\n 3    충청북도             7        8             789              736\\n 4    충청남도            10       10           2,508            1,731\\n 5    전라북도            19       19           2,626          10,703\\n 6    전라남도            13       13           2,962            6,917\\n 7    경상북도            18       18            2709            3,314\\n 8    경상남도             8       10            미정              1,408\\n 9    제주특별자치도         23       24           4,433            7,343\\n10    서울특별시           31       31          10,005            9,628\\n11    부산광역시           36       35           3,017            2,355\\n12    대구광역시           11       11             316              321\\n13    인천광역시           26       25           5,516            5,008\\n14    광주광역시           22       26           3,487            6,459\\n15    대전광역시           38       44            3685            3,848\\n16    울산광역시           17       14           1,302              660\\n17    세종특별자치시          8        9              96              373\\n        합계           322      322         143,602          75,727\\n\\n\\n                             - 4 -\\n\"\n```\n:::\n:::\n\n\n\nOh, this is too long even for an example. But you can realize that there are many `\\n` characters in the text. Let's split the text by the newline character and look at the first 10 lines of the 4th page. `\\n` refers to a new line in the text. We can split the text into lines by using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package, which is part of [`tidyverse`](https://www.tidyverse.org/). So, we don't need to load it separately. Let's look at the first six lines of the 4th page.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at the first 10 lines of the 4th page\npdf_text_all[4] |> \n  # Split by newline character.\n  str_split(\"\\n\") |> \n  # Unlist\n  unlist() |>\n  # Take the first 10 lines\n  head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"참고       기관별 사업규모 및 예산\"                                         \n [2] \"[중앙행정기관]\"                                                             \n [3] \"                  ‘24년     ‘23년      ‘24년 예산         ‘23년 예산\"       \n [4] \"       기관명\"                                                              \n [5] \"                  사업수      사업수         (백만원)           (백만원)\"   \n [6] \" 1    교육부             16       16          194,996          94,963\"      \n [7] \" 2    과학기술정보통신부        6        6           32,852          40,283\"\n [8] \" 3    외교부             73       63           40,215          39,419\"      \n [9] \"3-1   한국국제교류재단        37       41           42,514          44,664\" \n[10] \" 4    통일부              6        6            1,831           2,386\"      \n```\n:::\n:::\n\n\n\nThe 4th page in the pdf file looks like this:\n\n![2024 Public Diplomacy Comprehensive Implementation Plan, p. 4](data/2024pd4.png)\n\n## pdf Table Extraction\n\nLet's try to extract the second table on page 4 of the pdf file. The table has the number of public diplomacy projects and budgets for [first-tier local administration unit](https://www.mois.go.kr/eng/sub/a03/citiesprovinces/screen.do) (hereafter, *province_city* for short) in Korea. We will unlist each line as we did earlier so that we can see the table in a more readable way.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Look at the first 10 lines of the 4th page\nlines_pdf_4 <- pdf_text_all[4] |> \n  # Split by newline character.\n  str_split(\"\\n\") |> \n  # Unlist\n  unlist()\n```\n:::\n\n\n\nFirst, let's look at the 29^th^ and 30^th^ lines for the column names in the pdf file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlines_pdf_4[29:30]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"                  ‘24년     ‘23년      ‘24년 예산         ‘23년 예산\"\n[2] \"       기관명\"                                                       \n```\n:::\n:::\n\n\n\nThe column names are the line number, province or city's name, project numbers for 2024 and 2023 respectively, and the budget for 2024 and 2023 in million Korean Won respectively. Let's use the following English column names that correspond to the Korean column names in the pdf file.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Column names\ncol_names <- c(\"no\", \"province_city\", \"project_no_2024\", \"project_no_2023\", \"budget_2024\", \"budget_2023\")\n```\n:::\n\n\n\nBy observing the `lines_pdf_4` object using `view(lines_pdf_4)`, we can see that the second table starts from the 32^nd^ line and ends on the 48^th^. We will extract only those lines. We will use [`str_trim`](https://stringr.tidyverse.org/reference/str_trim.html) \"removes whitespace from start and end of string\". We will also use [`str_replace_all`](https://stringr.tidyverse.org/reference/str_replace.html) to remove commas from each line to convert entries into numbers. We will then split each line based on two or more consecutive spaces (our string is \"`\\s{2,}`\") using [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) and simplify the result into a matrix. We will convert this matrix into a data frame with non-factor columns using `data.frame(stringsAsFactors = FALSE)`. We will set the column names of the data frame using the `col_names` vector that we created above. These explanations are also available in each step in the following code chunk.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select lines 32 to 48 from the lines_pdf_4 data frame\nprovince_city_pd <- lines_pdf_4[32:48] |>\n  # Trim whitespace from both ends of each element in the selected rows\n  str_trim() |>\n  # Replace all commas with an empty string in each element\n  str_replace_all(\",\", \"\") |>\n  # Split each element based on 2 or more consecutive spaces and simplify into a matrix\n  str_split(\"\\\\s{2,}\", simplify = TRUE) |>\n  # Convert the matrix into a data frame with non-factor columns\n  data.frame(stringsAsFactors = FALSE) |>\n  # Set column names for the data frame using the provided 'col_names' vector\n  setNames(col_names)\n```\n:::\n\n\n\nLet's rearrange the table (which is originally in alphabetical order) by descending order based on public diplomacy budgets in 2024.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprovince_city_pd |>\n  arrange(desc(budget_2024))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   no  province_city project_no_2024 project_no_2023 budget_2024 budget_2023\n1   8       경상남도               8              10        미정        1408\n2  17 세종특별자치시               8               9          96         373\n3   3       충청북도               7               8         789         736\n4   2 강원특별자치도              10              11       78593       11024\n5  13     인천광역시              26              25        5516        5008\n6   9 제주특별자치도              23              24        4433        7343\n7  15     대전광역시              38              44        3685        3848\n8  14     광주광역시              22              26        3487        6459\n9  12     대구광역시              11              11         316         321\n10 11     부산광역시              36              35        3017        2355\n11  6       전라남도              13              13        2962        6917\n12  7       경상북도              18              18        2709        3314\n13  5       전라북도              19              19        2626       10703\n14  4       충청남도              10              10        2508        1731\n15  1         경기도              25              14       21558        3899\n16 16     울산광역시              17              14        1302         660\n17 10     서울특별시              31              31       10005        9628\n```\n:::\n:::\n\n\n\nBut these province_city names are in Korean since the document was in Korean. Let's practice extracting a table from internet then to find English names for these Korean provinces or cities. As of May 6, 2024, [Wikipedia's list of South Korea's administrative divisions](https://en.wikipedia.org/wiki/Administrative_divisions_of_South_Korea) seems to be correct. Let's extract the table there.\n\n## html Table Extraction\n\nWe will use the [`rvest`](https://rvest.tidyverse.org/) package to extract the table from the Wikipedia page. We will use the [`read_html`](https://rvest.tidyverse.org/reference/read_html.html) function to read the html content of the Wikipedia page. We will then use the [`html_node`](https://rvest.tidyverse.org/reference/html_nodes.html) function to select the table we want to extract. You can refer to [`rvest`](https://rvest.tidyverse.org/) package for more information on how to extract what you want. We can use the xpath of the table we want to extract. You can find the xpath of the table by right-clicking on the table on the Wikipedia page and selecting \"Inspect\" or \"Inspect Element\" depending on your browser. You can then right-click on the highlighted html element in the \"Elements\" tab of the \"Developer Tools\" and select \"Copy\" -\\> \"Copy XPath\". The xpath of the table we want to extract is `//*[@id=\"mw-content-text\"]/div[1]/table[5]`. We will use the [`html_table`](https://rvest.tidyverse.org/reference/html_table.html) function to extract the table as a data frame. We will use the `fill = TRUE` argument to fill in the missing values in the table.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhtml <- read_html(\"https://en.wikipedia.org/wiki/Administrative_divisions_of_South_Korea\")\n\ntable <- html |> \n  html_node(xpath = '//*[@id=\"mw-content-text\"]/div[1]/table[5]') |>\n  html_table(fill = TRUE)\n```\n:::\n\n\n\nLet's look at the first 10 rows of the table.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(table)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 9\n  Code  Emblem Name   Official English nam~1 Hangul Hanja Population 2020 Cens~2\n  <chr> <lgl>  <chr>  <chr>                  <chr>  <chr> <chr>                 \n1 KR-11 NA     Seoul~ Seoul                  서울~  .mw-~ 9,586,195             \n2 KR-26 NA     Busan~ Busan                  부산~  釜山~ 3,349,016             \n3 KR-27 NA     Daegu~ Daegu                  대구~  大邱~ 2,410,700             \n4 KR-28 NA     Inche~ Incheon                인천~  仁川~ 2,945,454             \n5 KR-29 NA     Gwang~ Gwangju                광주~  光州~ 1,477,573             \n6 KR-30 NA     Daeje~ Daejeon                대전~  大田~ 1,488,435             \n# i abbreviated names: 1: `Official English name[5]`,\n#   2: `Population 2020 Census`\n# i 2 more variables: `Area (km2)` <chr>,\n#   `Population density  2022 (per km2)` <chr>\n```\n:::\n:::\n\n\n\nPerfect! Now, let's keep only the columns that we will need.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select columns 4 and 5 from the table\ntable <- table |>\n  select(4:5)\n\n# Let's change the English province_city column name.\n\ntable <- table |>\n  rename(province_city_eng = `Official English name[5]`)\n```\n:::\n\n\n\nLet's hope that the Korean names in the Wikipedia table and the MOFA's pdf file are the same. Let's merge the two tables based on the Korean names.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Merge the two tables based on the Korean names\nprovince_city_pd_joined <- province_city_pd |>\n  left_join(table, by = c(\"province_city\" = \"Hangul\"))\n```\n:::\n\n\n\nLet's see if we have any missing values in the English names.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for missing values in the English names\nprovince_city_pd_joined |>\n  filter(is.na(province_city_eng))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  no province_city project_no_2024 project_no_2023 budget_2024 budget_2023\n1  5      전라북도              19              19        2626       10703\n  province_city_eng\n1              <NA>\n```\n:::\n:::\n\n\n\nWe almost got it! The only difference is 전라북도 (North Jeolla Province) in the MOFA's pdf file which is written as 전북특별자치도 (Jeonbuk State) in the Wikipedia table. Let's fix this.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Move the English name column next to the Korean name column, and remove the 'no' column\n\nprovince_city_pd_joined <- province_city_pd_joined |>\n  select(province_city, province_city_eng, everything(), -no)\n\n# Fix the English name of 전라북도\n\nprovince_city_pd_joined <- province_city_pd_joined |>\n  mutate(province_city_eng = ifelse(province_city == \"전라북도\", \"North Jeolla province_city\", province_city_eng))\n```\n:::\n\n\n\n## Text Analysis\n\n### Word Frequency\n\nThis time let's look at all of the text in the 2024 Public Diplomacy Comprehensive Implementation Plan. We will combine all the text into a single character vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine text\npdf_text <- str_c(pdf_text_all, collapse = \" \")\n```\n:::\n\n\n\nWe will now split the text into words using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package. We will then convert the result into a data frame with non-factor columns using the [`data.frame(stringsAsFactors = FALSE)`](https://rdocumentation.org/packages/base/versions/3.6.2) function. We will set the column name of the data frame as `word`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the text into words\nwords <- pdf_text |>\n  # Split the text into words\n  str_split(\"\\\\s+\") |>\n  # Convert the result into a data frame with non-factor columns\n  data.frame(stringsAsFactors = FALSE) |>\n  # Set the column name of the data frame as \"word\"\n  setNames(\"word\")\n```\n:::\n\n\n\nLet's look at the first 10 rows of the data frame.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(words, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           word\n1              \n2        2024년\n3      공공외교\n4  종합시행계획\n5          개요\n6             \n7          수립\n8          근거\n9             ❑\n10   공공외교법\n```\n:::\n:::\n\n\n\nNow, let's count the frequency of each word in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package. We will then arrange the result in descending order based on the frequency of the words.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of each word\nword_freq <- words |>\n  count(word, sort = TRUE)\n```\n:::\n\n\n\nLet's look at the first 10 rows of the data frame\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(word_freq, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       word  n\n1        및 72\n2         - 55\n3  공공외교 40\n4        등 33\n5        ㅇ 28\n6     개최, 22\n7      통한 22\n8      사업 18\n9      제고 18\n10     강화 17\n```\n:::\n:::\n\n\n\nThis is not very useful. There are two main issues with Korean text. First, Korean text does not have consistent spacing between words. Second, Korean text has particles and other morphemes that are not words. We will address these issues now.\n\n### Spacing in Korean Text\n\nLet's get the spacing right in Korean text using the [`bitNLP`](https://r2bit.com/bitNLP) package's [`get_spacing`](https://r2bit.com/bitNLP/reference/get_spacing.html) function, which will add spaces between words in the Korean text. So, for example \"한국공공외교\" will become \"한국 공공 외교\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the spacing right in Korean text\npdf_text_ko <- get_spacing(pdf_text)\n```\n:::\n\n\n\nNow, let's split the text into words again using the [`str_split`](https://stringr.tidyverse.org/reference/str_split.html) function from the [`stringr`](https://stringr.tidyverse.org/) package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the text into words\nwords_ko <- pdf_text_ko |>\n  # Split the text into words\n  str_split(\"\\\\s+\") |>\n  # Convert the result into a data frame with non-factor columns\n  data.frame(stringsAsFactors = FALSE) |>\n  # Set the column name of the data frame as \"word\"\n  setNames(\"word\")\n```\n:::\n\n\n\nLet's analyze the word frequency in the text again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of each word\nword_freq_ko <- words_ko |>\n  count(word, sort = TRUE)\n\nhead(word_freq_ko, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   word   n\n1     ▴ 175\n2     (  97\n3     -  80\n4    및  73\n5  외교  67\n6  공공  62\n7  국제  36\n8  사업  35\n9    등  33\n10 해외  30\n```\n:::\n:::\n\n\n\nWe have many special characters in the text. Let's remove all characters except for Korean characters, spaces, English letters, and numbers using the [`str_replace_all`](https://stringr.tidyverse.org/reference/str_replace.html) function from the [`stringr`](https://stringr.tidyverse.org/) package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Remove all characters except for Korean characters, spaces, English letters, and numbers\nword_freq_ko <- pdf_text_ko |>\n  # Remove all characters except Korean characters, English letters, numbers, and spaces\n  str_replace_all(\"[^가-힣a-zA-Z0-9\\\\s]\", \"\") |>\n  # Split the cleaned text into words based on one or more spaces\n  str_split(\"\\\\s+\") |>\n  # Convert the list result into a data frame with non-factor columns\n  data.frame(stringsAsFactors = FALSE) |>\n  # Set the column name of the data frame as \"word\"\n  setNames(\"word\")\n```\n:::\n\n\n\nLet's analyze the word frequency in the text again.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of each word\nword_freq_ko <- word_freq_ko |>\n  count(word, sort = TRUE)\n\nhead(word_freq_ko, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   word  n\n1    및 73\n2  외교 67\n3  공공 62\n4  개최 44\n5  사업 37\n6  국제 36\n7    등 35\n8  해외 30\n9  문화 29\n10 추진 28\n```\n:::\n:::\n\n\n\nThis is much better! We have removed the special characters and have more meaningful words in the text. Let's move on to morpheme analysis which makes more sense in Korean text analysis context.\n\n### Morpheme Analysis in Korean Text\n\nLet's analyze the morphemes in the Korean text using the [`morpho_mecab`](https://r2bit.com/bitNLP/reference/morpho_mecab.html) function from the [`bitNLP`](https://r2bit.com/bitNLP) package, which will extract morphemes from the Korean text.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Analyze the morphemes in the Korean text\nmorphemes <- morpho_mecab(pdf_text_ko)\n```\n:::\n\n\n\nThis creates a list of character vectors, where each element corresponds to a morpheme in the text. We can also combine all of the morphemes and tokenize them into a single character vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine all the morphemes into a single character vector\n\nmorphemes_single <- morpho_mecab(pdf_text_ko, indiv = FALSE)\n```\n:::\n\n\n\nNow, let's split the text into words again this time by converting `morphemes_single` into a data frame using the [`as.data.frame`](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/as.data.frame) function. We will set the column name of the data frame as \"word\".\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Split the text into words\nwords_morphemes <- morphemes_single |>\n  as.data.frame() |>\n  # Set the column name of the data frame as \"word\"\n  setNames(\"word\")\n```\n:::\n\n\n\nWe will now count the frequency of each morpheme in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package. We will then arrange the result in descending order based on the frequency of the morphemes.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count the frequency of each morpheme\n\nmorpheme_freq <- words_morphemes |>\n  count(word, sort = TRUE)\n\nhead(morpheme_freq, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   word  n\n1  외교 68\n2  공공 62\n3  개최 46\n4  국제 39\n5  사업 37\n6  해외 30\n7  문화 29\n8  추진 28\n9  운영 26\n10 계획 25\n```\n:::\n:::\n\n\n\nNow, this is more like it!\n\nLet's visualize the frequency of the morphemes in the text using a bar plot. We will use the [`ggplot`](https://ggplot2.tidyverse.org/) function from the [`ggplot2`](https://ggplot2.tidyverse.org/) package to create the plot. We will use the [`geom_col`](https://ggplot2.tidyverse.org/reference/geom_col.html) function to add the bars to the plot. We will use the [`theme_minimal`](https://ggplot2.tidyverse.org/reference/theme_minimal.html) function to set the theme of the plot to minimal. We will use the [`theme`](https://ggplot2.tidyverse.org/reference/theme.html) function to adjust the font size in the plot. We will set the font size to 10. We will use the [`labs`](https://ggplot2.tidyverse.org/reference/labs.html) function to add the title and labels to the plot.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visualize the frequency of the morphemes in the text\n\nmorpheme_freq |>\n  # Create a bar plot\n  ggplot(aes(x = reorder(word, n), y = n)) +\n  geom_col(fill = \"#2196f3\") +\n  theme_minimal() +\n  theme(text = element_text(size = 10)) +\n  labs(title = \"Frequency of Morphemes in the 2024 Public Diplomacy Comprehensive Implementation Plan\",\n       x = \"Morpheme\",\n       y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](text_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Word Network in Korean Text\n\nLet's analyze the word network in the Korean text using the [`tokenize_noun_ngrams`](https://r2bit.com/bitNLP/reference/tokenize_noun_ngrams.html) function from the [`bitNLP`](https://r2bit.com/bitNLP/) package which builds on [`tidytext`](https://r2bit.com/bitNLP/articles/with_tidytext.html) package. We will use the `tokenize_noun_grams` function to extract the noun word network from the Korean text.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We can use a user-defined dictionary to improve the accuracy of the tokenization. We will rely on the one provided by the `bitNLP` package.\n\ndic_path <- system.file(\"dic\", package = \"bitNLP\")\ndic_file <- glue::glue(\"{dic_path}/buzz_dic.dic\")\n\nword_network <- tokenize_noun_ngrams(pdf_text_ko, simplify = TRUE, user_dic = dic_file, n = 2) |>\n  as_tibble() |>\n  setNames(\"paired_words\")\n```\n:::\n\n\n\nNow, let's separate the paired words into two columns using the [`separate`](https://tidyr.tidyverse.org/reference/separate.html) function from the [`tidyr`](https://tidyr.tidyverse.org/) package which is loaded as part of the [`tidyverse`](https://www.tidyverse.org/) package. This will allow us to create bigrams from the paired words.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_network_separated <- word_network |>\n  separate(paired_words, c(\"word1\", \"word2\"), sep = \" \")\n```\n:::\n\n\n\nWe will now count the frequency of each bigram in the text using the [`count`](https://dplyr.tidyverse.org/reference/count.html) function from the [`dplyr`](https://dplyr.tidyverse.org/) package package, which is also party of the [`tidyverse`](https://www.tidyverse.org/). We will then arrange the result in descending order based on the frequency of the bigrams.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# new bigram counts:\nword_network_counts <- word_network_separated |>\n  count(word1, word2, sort = TRUE)\n```\n:::\n\n\n\nKorean text sometimes is not visible in the graph due to the font issue. This was the case in my Macbook. Let's set the font to one that supports Korean characters. We will use the [`extrafont`](https://github.com/wch/extrafont) package to set the font to one that supports Korean characters. We will use the [`font_import`](https://github.com/wch/extrafont) function to import the fonts from the system. This may take some time. You only need to do it once. That's why I commented it. You can uncomment it in first usage. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load extrafont and register fonts\n\n#font_import()  # This might take a while if it's the first time you're running it\n```\n:::\n\n\n\nWe will then use the [`loadfonts`](https://github.com/wch/extrafont) function to load the fonts. We will use the [`fonts`](https://github.com/wch/extrafont) function to display the available fonts and find one that supports Korean characters. We will set the font to one that supports Korean characters. For now, I have chosen \"Arial Unicode MS\" as the Korean font. You can replace it with a font from your system that supports Korean characters if necessary.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#loadfonts(device = \"all\")\n\n# Display available fonts, find one that supports Korean\n#fonts()\n\n# Set the font to one that supports Korean characters\nkorean_font <- \"Arial Unicode MS\"  # Replace with a font from your system that supports Korean if necessary\n```\n:::\n\n\n\nWe will now create a graph from the bigram counts using the [`graph_from_data_frame`](https://r.igraph.org/reference/graph_from_data_frame.html) function from the [`igraph`](https://r.igraph.org/) package. We will use the [`ggraph`](https://ggraph.data-imaginist.com/reference/ggraph.html) function from the [`ggraph`](https://ggraph.data-imaginist.com) package to create the graph. We will use the [`geom_edge_link`](https://ggraph.data-imaginist.com/reference/geom_edge_link.html) function to add the edges to the graph. We will use the [`geom_node_point`](https://ggraph.data-imaginist.com/reference/geom_node_point.html) function to add the nodes to the graph. We will use the [`geom_node_text`](https://ggraph.data-imaginist.com/reference/geom_node_text.html) function to add the labels to the nodes in the graph. We will set the font to the Korean font that we set earlier. We will then adjust the font in the graph. Here, `n >= 6` is used to filter out bigrams that appear less than 6 times. You can adjust this number as needed. You can check out `ggraph` layout options [here](https://cran.r-project.org/web/packages/ggraph/vignettes/Layouts.html).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nword_network_select <- word_network_counts |>\n  filter(n >= 6) |>\n  graph_from_data_frame() |>\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(color = \"#2196f3\", size = 4) +\n  geom_node_text(aes(label = name), family = korean_font, vjust = 2, size = 4) +  # Set family to Korean font\n  theme_void()\n\n\nword_network_select\n```\n\n::: {.cell-output-display}\n![](text_files/figure-pdf/unnamed-chunk-39-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Sentiment Analysis\n\n### Topic Modeling\n\n## Korean Tweet Analysis\n\n## Further Readings\n\n## References\n\n## Session Info\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.4.0 (2024-04-24)\nPlatform: aarch64-apple-darwin20\nRunning under: macOS Sonoma 14.4.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Asia/Seoul\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] extrafont_0.19    ggraph_2.2.1      igraph_2.0.3      tidytext_0.4.2   \n [5] rvest_1.0.4       pdftools_3.4.0    lubridate_1.9.3   forcats_1.0.0    \n [9] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n[13] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n[17] bitNLP_1.4.3.9000\n\nloaded via a namespace (and not attached):\n [1] tidyselect_1.2.1    viridisLite_0.4.2   farver_2.1.1       \n [4] viridis_0.6.5       fastmap_1.1.1       tweenr_2.0.3       \n [7] janeaustenr_1.0.0   promises_1.3.0      shinyjs_2.1.0      \n[10] digest_0.6.35       timechange_0.3.0    mime_0.12          \n[13] lifecycle_1.0.4     qpdf_1.3.3          tokenizers_0.3.0   \n[16] magrittr_2.0.3      compiler_4.4.0      rlang_1.1.3        \n[19] sass_0.4.9          tools_4.4.0         utf8_1.2.4         \n[22] knitr_1.46          labeling_0.4.3      askpass_1.2.0      \n[25] graphlayouts_1.1.1  htmlwidgets_1.6.4   curl_5.2.1         \n[28] xml2_1.3.6          miniUI_0.1.1.1      ngram_3.2.3        \n[31] withr_3.0.0         grid_4.4.0          polyclip_1.10-6    \n[34] fansi_1.0.6         xtable_1.8-4        colorspace_2.1-0   \n[37] extrafontdb_1.0     scales_1.3.0        MASS_7.3-60.2      \n[40] tinytex_0.50        cli_3.6.2           rmarkdown_2.26     \n[43] generics_0.1.3      RcppParallel_5.1.7  rstudioapi_0.16.0  \n[46] httr_1.4.7          tzdb_0.4.0          cachem_1.0.8       \n[49] ggforce_0.4.2       RcppMeCab_0.0.1.2   parallel_4.4.0     \n[52] rhandsontable_0.3.8 vctrs_0.6.5         Matrix_1.7-0       \n[55] jsonlite_1.8.8      hms_1.1.3           ggrepel_0.9.5      \n[58] jquerylib_0.1.4     shinyBS_0.61.1      glue_1.7.0         \n[61] stringi_1.8.3       gtable_0.3.5        later_1.3.2        \n[64] munsell_0.5.1       pillar_1.9.0        htmltools_0.5.8.1  \n[67] R6_2.5.1            tidygraph_1.3.1     evaluate_0.23      \n[70] shiny_1.8.1.1       lattice_0.22-6      SnowballC_0.7.1    \n[73] memoise_2.0.1       DataEditR_0.1.5     httpuv_1.6.15      \n[76] bslib_0.7.0         Rcpp_1.0.12         Rttf2pt1_1.3.12    \n[79] gridExtra_2.3       xfun_0.43           pkgconfig_2.0.3    \n```\n:::\n:::\n",
    "supporting": [
      "text_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}